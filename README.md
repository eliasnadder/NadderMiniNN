# NadderMiniNN - Mini Neural Network Library

A mini neural network library built from scratch using Python and NumPy only.

Ù…ÙƒØªØ¨Ø© Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙˆÙ†ÙŠØ© Ù…ØµØºØ±Ø© Ù…Ø¨Ù†ÙŠØ© Ù…Ù† Ø§Ù„ØµÙØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Python Ùˆ NumPy ÙÙ‚Ø·.

## ğŸ“ Project Structure / Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

```txt
NadderMiniNN/
â”œâ”€â”€ NadderMiniNN/
â”‚   â”œâ”€â”€ classes                         # All layers (Dense, Activations, etc.)
â”‚   â”‚   â”œâ”€â”€ Activitions 
â”‚   â”‚   â”‚   â”œâ”€â”€ BatchNormalization.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Dropout.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Linear.py
â”‚   â”‚   â”‚   â”œâ”€â”€ MeanSquaredError.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Relu.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Sigmoid.py
â”‚   â”‚   â”‚   â”œâ”€â”€ SoftmaxWithLoss.py
â”‚   â”‚   â”‚   â””â”€â”€ Tanh.py
â”‚   â”‚   â”‚   
â”‚   â”‚   â”œâ”€â”€ Optimizers                  # Optimization algorithms (SGD, Adam, etc.)
â”‚   â”‚   â”‚   â”œâ”€â”€ AdaGrad.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Adam.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Momentum.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Optimizer
â”‚   â”‚   â”‚   â”œâ”€â”€ RMSprop.py
â”‚   â”‚   â”‚   â””â”€â”€ SGD.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Dense.py
â”‚   â”‚   â””â”€â”€ Layer.py
â”‚   â”‚   
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ neural_network.py               # Basic neural network structure
â”‚   â”œâ”€â”€ trainer.py                      # Network trainer
â”‚   â””â”€â”€ hyperparameter_tuning.py        # Hyperparameter tuning
â”‚   
â”œâ”€â”€ examples/                           # Example usage of the library
â”‚   â”œâ”€â”€ example_iris.py
â”‚   â””â”€â”€ example_mnist.py
â”‚
â”œâ”€â”€ tests/                              # Tests
â”‚   â””â”€â”€ test_library.py
â”‚
â”œâ”€â”€ setup.py 
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ MANIFEST.in
â””â”€â”€ .gitignore
```

## ğŸš€ Features / Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª

### Available Layers / Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

- **Dense**: Fully Connected Layer
- **Activation Functions** / Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙØ¹ÙŠÙ„:
  - Linear
  - ReLU
  - Sigmoid
  - Tanh
- **Regularization** / Ø§Ù„ØªÙ†Ø¸ÙŠÙ…:
  - Dropout
  - Batch Normalization
- **Loss Functions** / Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©:
  - Mean Squared Error
  - Softmax with Cross Entropy

### Optimization Algorithms / Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†

- SGD (Stochastic Gradient Descent)
- Momentum
- AdaGrad
- Adam
- RMSprop

### Additional Features / Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©

- Complete training system with accuracy and loss tracking
- Hyperparameter tuning (Grid Search & Random Search)
- Batch Normalization and Dropout support
- Weight initialization (He, Xavier)

## ğŸ’» Requirements / Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„

```bash
numpy>=1.19.0
scikit-learn>=0.24.0  # Only for the example
```

## ğŸ“– Usage / ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

### Simple Example / Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·

```python
from layers import Dense, Relu, SoftmaxWithLoss
from neural_network import NeuralNetwork
from optimizers import Adam
from trainer import Trainer

# Build the network
network = NeuralNetwork()
network.add_layer('dense1', Dense(4, 16))
network.add_layer('relu1', Relu())
network.add_layer('dense2', Dense(16, 3))
network.set_loss_layer(SoftmaxWithLoss())
network.init_weights('he')

# Training
optimizer = Adam(lr=0.01)
trainer = Trainer(network, optimizer)
trainer.fit(X_train, y_train, X_test, y_test, epochs=50, batch_size=32)

# Prediction
predictions = network.predict(X_test, train_mode=False)
```

### Complex Network / Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ù…Ø¹Ù‚Ø¯Ø©

```python
# Dense > Sigmoid > BatchNorm > Dense > Relu > Dense > SoftmaxWithLoss
network = NeuralNetwork()
network.add_layer('dense1', Dense(input_size, 64))
network.add_layer('sigmoid1', Sigmoid())
network.add_layer('batchnorm1', BatchNormalization(64))
network.add_layer('dense2', Dense(64, 32))
network.add_layer('relu1', Relu())
network.add_layer('dense3', Dense(32, num_classes))
network.set_loss_layer(SoftmaxWithLoss())
```

### Hyperparameter Tuning / Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø©

```python
from hyperparameter_tuning import HyperparameterTuning

# Define a function to build the network
def network_builder(hidden_size=32, **kwargs):
    network = NeuralNetwork()
    network.add_layer('dense1', Dense(4, hidden_size))
    network.add_layer('relu1', Relu())
    network.add_layer('dense2', Dense(hidden_size, 3))
    network.set_loss_layer(SoftmaxWithLoss())
    network.init_weights('he')
    return network

# Define a function to build the trainer
def trainer_builder(network, lr=0.01, **kwargs):
    optimizer = Adam(lr=lr)
    return Trainer(network, optimizer)

# Search for best parameters
tuner = HyperparameterTuning(network_builder, trainer_builder)
param_grid = {
    'hidden_size': [16, 32, 64],
    'lr': [0.001, 0.01, 0.1]
}
best_params = tuner.grid_search(param_grid, X_train, y_train, X_val, y_val)
```

## ğŸ§ª Running the Example / ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„

```bash
python example.py
```

This example does the following:

1. Load Iris dataset
2. Build a neural network
3. Train the network
4. Display results and accuracy

## ğŸ“Š Example Output / Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø±Ø¬

```txt
============================================================
Testing NadderMiniNN Library on Iris Dataset
============================================================

Loading and preparing data...
Training samples: 120
Testing samples: 30
Features: 4
Classes: 3

Building network...
Network architecture:
  dense1: Dense
  sigmoid1: Sigmoid
  batchnorm1: BatchNormalization
  dense2: Dense
  relu1: Relu
  dense3: Dense

Starting training...
------------------------------------------------------------
Epoch 10/100 - Loss: 0.8234 - Train Acc: 0.6667 - Test Acc: 0.6667
Epoch 20/100 - Loss: 0.4521 - Train Acc: 0.8750 - Test Acc: 0.9000
...
Epoch 100/100 - Loss: 0.1234 - Train Acc: 0.9833 - Test Acc: 0.9667

============================================================
Final Results:
============================================================
Final Train Accuracy: 0.9833
Final Test Accuracy: 0.9667
```

## ğŸ—ï¸ Architecture / Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©

### Forward Propagation / Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ

Each layer implements a `forward(x)` operation that computes the output from the input.

### Backward Propagation / Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø¹ÙƒØ³ÙŠ

Each layer implements a `backward(dout)` operation that computes the gradients.

### Weight Update / ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†

The Optimizer uses the computed gradients to update the network weights.

## ğŸ“ Important Notes / Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©

1. **Weight Initialization**: Use `he` for ReLU and `xavier` for Sigmoid/Tanh
2. **Batch Normalization**: Improves training speed and stability
3. **Dropout**: Helps prevent Overfitting
4. **Learning Rate**: Start with small values (0.001 - 0.01) with Adam

## ğŸ”§ Extension / Ø§Ù„ØªÙˆØ³Ø¹Ø©

You can easily add:

- New layers by inheriting from `Layer` class
- New Optimizers by inheriting from `Optimizer` class
- New Loss functions

Example:

```python
class MyCustomLayer(Layer):
    def forward(self, x):
        # Your implementation here
        return output
    
    def backward(self, dout):
        # Your implementation here
        return dx
```

## ğŸ“„ License / Ø§Ù„ØªØ±Ø®ÙŠØµ

This project is open source for academic use.

## âœï¸ Author / Ø§Ù„Ù…Ø¤Ù„Ù

Elias Nadder - Damascus University - ITE - Fourth Year

---

**Note**: This library is designed for educational purposes to understand how neural networks work internally.
